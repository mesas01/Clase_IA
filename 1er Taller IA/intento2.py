import numpy as np


# 1. Funci贸n para calcular la hip贸tesis h_(X)
def calculate_hypothesis(X, theta):
    """
    Calcula la hip贸tesis h_(X) para un conjunto de datos X y par谩metros .

    Par谩metros:
    - X: Matriz de entrada (m x n), donde m es el n煤mero de ejemplos y n el n煤mero de caracter铆sticas.
    - theta: Vector de par谩metros (n+1 x 1), incluyendo el t茅rmino de sesgo (bias).

    Retorna:
    - La hip贸tesis calculada para cada ejemplo de entrada.
    """
    return np.dot(X, theta)


# 2. Funci贸n para calcular el valor de la funci贸n de costo J()
def compute_cost(X, y, theta):
    """
    Calcula el valor de la funci贸n de costo J() para una hip贸tesis dada y las etiquetas reales.

    Par谩metros:
    - X: Matriz de entrada.
    - y: Vector de etiquetas reales.
    - theta: Vector de par谩metros.

    Retorna:
    - El valor de la funci贸n de costo.
    """
    m = len(y)  # N煤mero de ejemplos de entrenamiento
    h = calculate_hypothesis(X, theta)
    return (1 / (2 * m)) * np.sum((h - y) ** 2)


# 3. Funci贸n para calcular el gradiente de la funci贸n de costo
def compute_gradient(X, y, theta):
    """
    Calcula el gradiente de la funci贸n de costo respecto a los par谩metros .

    Par谩metros:
    - X: Matriz de entrada.
    - y: Vector de etiquetas reales.
    - theta: Vector de par谩metros.

    Retorna:
    - El gradiente de la funci贸n de costo.
    """
    m = len(y)
    h = calculate_hypothesis(X, theta)
    return (1 / m) * np.dot(X.T, (h - y))


# 4. Funci贸n para actualizar los par谩metros 
def update_parameters(theta, learning_rate, gradient):
    """
    Actualiza los par谩metros  utilizando el gradiente de la funci贸n de costo.

    Par谩metros:
    - theta: Vector de par谩metros actual.
    - learning_rate: Tasa de aprendizaje.
    - gradient: Gradiente de la funci贸n de costo.

    Retorna:
    - Los par谩metros  actualizados.
    """
    return theta - learning_rate * gradient

# Incorporar estas funciones en el proceso de entrenamiento y visualizaci贸n puede seguir un enfoque similar
# al mostrado anteriormente, donde se llaman estas funciones en lugar de tener el c贸digo directamente
# en el bucle del gradiente descendiente o en la funci贸n de visualizaci贸n.
