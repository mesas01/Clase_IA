import numpy as np  # Para operaciones num칠ricas y manejo de arrays
import pandas as pd  # Para la manipulaci칩n y an치lisis de datos
from sklearn.model_selection import train_test_split  # Para dividir los datos en conjuntos de entrenamiento y prueba
from sklearn.preprocessing import StandardScaler  # Para la normalizaci칩n de los datos
import matplotlib.pyplot as plt  # Para la visualizaci칩n de datos
from ipywidgets import interact, FloatSlider  # Para crear interfaces interactivas en Jupyter notebooks


# Cargar los datos desde un archivo Excel
datosExcel = pd.read_excel('Real estate valuation data set.xlsx')

# observamos una fraccion de ellos
datosExcel.head(5)

# Seleccionar las caracter칤sticas y la variables de las casas
caracteristicas = ['X2 house age', 'X3 distance to the nearest MRT station', 'X4 number of convenience stores', 'X5 latitude', 'X6 longitude']
X = datosExcel[caracteristicas].values
Y = datosExcel['Y house price of unit area'].values.reshape(-1, 1)

# Crear una figura y una matriz de subgr치ficos con 2 filas y 3 columnas
fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))

# T칤tulos personalizados para cada gr치fico
nombres_caracteristicas = ['Edad de la casa', 'Distancia a la estaci칩n MRT m치s cercana', 'N칰mero de tiendas de conveniencia', 'Latitud', 'Longitud', 'Distribuci칩n de Precios']

# Llenar cada subgr치fico con un gr치fico de dispersi칩n para cada caracter칤stica
for i, ax in enumerate(axes.flat):
    if i < 5:  # Para las primeras 5 caracter칤sticas
        ax.scatter(datosExcel[caracteristicas[i]], datosExcel['Y house price of unit area'], alpha=0.4, edgecolor='none')
        ax.set_xlabel(caracteristicas[i])
        ax.set_ylabel('Precio [dolares/$m^2$]')
    else:  # Para el sexto gr치fico, mostramos la distribuci칩n de precios
        ax.hist(datosExcel['Y house price of unit area'], bins=20, color='skyblue', edgecolor='black')
        ax.set_xlabel('Precio [dolares/$m^2$]')
        ax.set_ylabel('Frecuencia')
    ax.set_title(nombres_caracteristicas[i])
    ax.grid(visible=True, alpha=0.2)

plt.tight_layout()
plt.show()

# funcion que Divide los datos en conjuntos de entrenamiento y prueba, y normaliza las caracter칤sticas.
def dividir_y_normalizar_datos(X, Y):

    # Dividir los datos en conjuntos de entrenamiento y prueba
    # test_size=0.2 indica que el 20% de los datos se utilizar치 como conjunto de prueba
    # random_state asegura que la divisi칩n sea reproducible
    X_entrenamiento, X_prueba, Y_entrenamiento, Y_prueba = train_test_split(X, Y, test_size=0.2, random_state=42)

    # Inicializar el objeto StandardScaler
    escalador_X = StandardScaler()

    # Ajustar el escalador solo a los datos de entrenamiento y transformarlos
    # Esto calcula la media y desviaci칩n est치ndar de cada caracter칤stica en el conjunto de entrenamiento
    # y luego utiliza estos valores para escalar el conjunto de entrenamiento
    X_entrenamiento_escalado = escalador_X.fit_transform(X_entrenamiento)

    # Transformar los datos de prueba utilizando la misma transformaci칩n aplicada a los datos de entrenamiento
    # Es importante no ajustar el escalador con los datos de prueba para evitar el sesgo
    X_prueba_escalado = escalador_X.transform(X_prueba)

    # A침adir una columna de unos al inicio de las matrices escaladas para el t칠rmino de sesgo (intercepto)

    X_entrenamiento_escalado = np.hstack([np.ones((X_entrenamiento_escalado.shape[0], 1)), X_entrenamiento_escalado])
    X_prueba_escalado = np.hstack([np.ones((X_prueba_escalado.shape[0], 1)), X_prueba_escalado])
    return X_entrenamiento_escalado, X_prueba_escalado, Y_entrenamiento, Y_prueba


def calcular_hipotesis(X, theta):
    """
    Calcula la hip칩tesis h_洧랚(X) para un conjunto de datos X y par치metros 洧랚.
    """
    return np.dot(X, theta)


def calcular_costo(X, Y, theta):
    """
    Calcula el valor de la funci칩n de costo J(洧랚).
    """
    m = len(Y)
    h = calcular_hipotesis(X, theta)
    return (1/(2*m)) * np.sum(np.square(h - Y))

def calcular_gradiente(X, Y, theta):
    """
    Calcula el gradiente de la funci칩n de costo.
    """
    m = len(Y)
    h = calcular_hipotesis(X, theta)
    return (1/m) * np.dot(X.T, (h - Y))

def actualizar_parametros(theta, tasa_aprendizaje, gradiente):
    """
    Actualiza los par치metros 洧랚 utilizando el gradiente de la funci칩n de costo.
    """
    return theta - tasa_aprendizaje * gradiente


def main():
    X_entrenamiento_escalado, X_prueba_escalado, Y_entrenamiento, Y_prueba = dividir_y_normalizar_datos(X, Y)

    theta_inicial = np.zeros((X_entrenamiento_escalado.shape[1], 1))

    tasa_aprendizaje = 0.01
    iteraciones = 1000

    theta = theta_inicial
    for i in range(iteraciones):
        gradiente = calcular_gradiente(X_entrenamiento_escalado, Y_entrenamiento, theta)
        theta = actualizar_parametros(theta, tasa_aprendizaje, gradiente)

    # Ejemplo de c칩mo se podr칤an imprimir los resultados del entrenamiento
    costo_final = calcular_costo(X_entrenamiento_escalado, Y_entrenamiento, theta)
    print("Costo final despu칠s del entrenamiento:", costo_final)
    print("Par치metros theta finales:", theta)


if __name__ == "__main__":
    main()
